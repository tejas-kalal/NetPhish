{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93u_rVz4HwN1",
        "outputId": "aa1a4be1-9d44-4585-edfd-64523d1cf554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy0hclgCI281",
        "outputId": "05e4ee8f-698f-4aa3-83c5-7199036a9363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3->pytorch-tabnet)\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata\u001b[0m\u001b[33m\n",
            "\u001b[0m  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/nvidia-cusolver-cu12/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcEREuaVI-wL",
        "outputId": "dbeb7853-38a4-424e-dc84-331e1c45c3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.6.0+cu124\n",
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRLld8iTHLhU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netphish = pd.read_csv('/content/NetPhishV4.csv')"
      ],
      "metadata": {
        "id": "7ihGiN5zJJwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Step 1: Don't convert to .values so we keep column names\n",
        "X = netphish.drop('class', axis=1)\n",
        "y = netphish['class']\n",
        "\n",
        "# Step 2: Encode target\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Step 3: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# Step 4: Train-test split\n",
        "X_train_full, X_test_full, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: RFE with RandomForest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "rfe = RFE(model, n_features_to_select=20)\n",
        "rfe.fit(X_train_full, y_train)\n",
        "\n",
        "# Step 6: Select top features\n",
        "selected_features = X.columns[rfe.support_]\n",
        "X_train = X_train_full[selected_features]\n",
        "X_test = X_test_full[selected_features]\n",
        "\n",
        "# Step 7: Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=64)"
      ],
      "metadata": {
        "id": "iaALFbehHTmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.out = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "model = MLP(X_train.shape[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test_tensor)\n",
        "    y_pred = preds.argmax(dim=1).numpy()\n",
        "\n",
        "print(\"MLP Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NodSd7viJVcS",
        "outputId": "49636fbf-b48d-4e55-f083-86771d50e862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Performance:\n",
            "Accuracy: 0.9371\n",
            "Precision: 0.9448\n",
            "Recall: 0.9279\n",
            "F1 Score: 0.9363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(CNN1D, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3)\n",
        "        self.fc1 = nn.Linear((input_dim - 4) * 32, 64)\n",
        "        self.out = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add channel dimension\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.out(x)\n",
        "\n",
        "cnn_model = CNN1D(X_train.shape[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training CNN\n",
        "for epoch in range(20):\n",
        "    cnn_model.train()\n",
        "    for xb, yb in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        preds = cnn_model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "cnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = cnn_model(X_test_tensor)\n",
        "    y_pred = preds.argmax(dim=1).numpy()\n",
        "\n",
        "print(\"1D CNN Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYObRaWRJcP5",
        "outputId": "53455159-f256-4c92-d621-2207cab80ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1D CNN Performance:\n",
            "Accuracy: 0.9463\n",
            "Precision: 0.9523\n",
            "Recall: 0.9392\n",
            "F1 Score: 0.9457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tabnet_clf = TabNetClassifier()\n",
        "\n",
        "tabnet_clf.fit(\n",
        "    X_train=X_train.values,           # convert DataFrame to NumPy array\n",
        "    y_train=y_train,                  # if y_train is already np.array or Series, it's fine\n",
        "    eval_set=[(X_test.values, y_test)],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=100,\n",
        "    patience=10,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "y_pred = tabnet_clf.predict(X_test.values)\n",
        "\n",
        "print(\"TabNet Performance:\")\n",
        "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYYg7NMoJc0v",
        "outputId": "f4a1777b-ff84-4d85-d26a-20928f2114a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.46767 | val_0_accuracy: 0.86769 |  0:00:03s\n",
            "epoch 1  | loss: 0.32677 | val_0_accuracy: 0.87848 |  0:00:05s\n",
            "epoch 2  | loss: 0.29461 | val_0_accuracy: 0.89095 |  0:00:07s\n",
            "epoch 3  | loss: 0.28127 | val_0_accuracy: 0.8953  |  0:00:09s\n",
            "epoch 4  | loss: 0.26281 | val_0_accuracy: 0.90361 |  0:00:10s\n",
            "epoch 5  | loss: 0.24828 | val_0_accuracy: 0.90807 |  0:00:12s\n",
            "epoch 6  | loss: 0.23748 | val_0_accuracy: 0.90747 |  0:00:14s\n",
            "epoch 7  | loss: 0.23431 | val_0_accuracy: 0.91222 |  0:00:16s\n",
            "epoch 8  | loss: 0.22705 | val_0_accuracy: 0.91687 |  0:00:18s\n",
            "epoch 9  | loss: 0.22277 | val_0_accuracy: 0.91549 |  0:00:20s\n",
            "epoch 10 | loss: 0.21611 | val_0_accuracy: 0.91094 |  0:00:22s\n",
            "epoch 11 | loss: 0.20832 | val_0_accuracy: 0.91677 |  0:00:23s\n",
            "epoch 12 | loss: 0.20609 | val_0_accuracy: 0.92261 |  0:00:25s\n",
            "epoch 13 | loss: 0.2031  | val_0_accuracy: 0.92053 |  0:00:27s\n",
            "epoch 14 | loss: 0.20131 | val_0_accuracy: 0.92301 |  0:00:29s\n",
            "epoch 15 | loss: 0.20141 | val_0_accuracy: 0.92281 |  0:00:31s\n",
            "epoch 16 | loss: 0.20086 | val_0_accuracy: 0.92865 |  0:00:33s\n",
            "epoch 17 | loss: 0.19613 | val_0_accuracy: 0.92222 |  0:00:34s\n",
            "epoch 18 | loss: 0.19147 | val_0_accuracy: 0.92548 |  0:00:36s\n",
            "epoch 19 | loss: 0.18651 | val_0_accuracy: 0.92697 |  0:00:39s\n",
            "epoch 20 | loss: 0.18762 | val_0_accuracy: 0.93003 |  0:00:41s\n",
            "epoch 21 | loss: 0.18465 | val_0_accuracy: 0.92974 |  0:00:43s\n",
            "epoch 22 | loss: 0.18189 | val_0_accuracy: 0.92825 |  0:00:44s\n",
            "epoch 23 | loss: 0.18381 | val_0_accuracy: 0.92914 |  0:00:46s\n",
            "epoch 24 | loss: 0.18196 | val_0_accuracy: 0.93162 |  0:00:48s\n",
            "epoch 25 | loss: 0.17919 | val_0_accuracy: 0.92796 |  0:00:50s\n",
            "epoch 26 | loss: 0.17771 | val_0_accuracy: 0.93478 |  0:00:52s\n",
            "epoch 27 | loss: 0.17773 | val_0_accuracy: 0.92707 |  0:00:54s\n",
            "epoch 28 | loss: 0.17755 | val_0_accuracy: 0.93122 |  0:00:56s\n",
            "epoch 29 | loss: 0.17575 | val_0_accuracy: 0.92954 |  0:00:58s\n",
            "epoch 30 | loss: 0.17369 | val_0_accuracy: 0.93577 |  0:00:59s\n",
            "epoch 31 | loss: 0.1706  | val_0_accuracy: 0.92885 |  0:01:01s\n",
            "epoch 32 | loss: 0.17189 | val_0_accuracy: 0.9338  |  0:01:03s\n",
            "epoch 33 | loss: 0.17113 | val_0_accuracy: 0.93221 |  0:01:05s\n",
            "epoch 34 | loss: 0.1705  | val_0_accuracy: 0.93102 |  0:01:07s\n",
            "epoch 35 | loss: 0.16961 | val_0_accuracy: 0.93488 |  0:01:09s\n",
            "epoch 36 | loss: 0.16623 | val_0_accuracy: 0.93449 |  0:01:11s\n",
            "epoch 37 | loss: 0.16776 | val_0_accuracy: 0.93449 |  0:01:12s\n",
            "epoch 38 | loss: 0.16574 | val_0_accuracy: 0.93112 |  0:01:14s\n",
            "epoch 39 | loss: 0.16469 | val_0_accuracy: 0.93795 |  0:01:16s\n",
            "epoch 40 | loss: 0.16504 | val_0_accuracy: 0.93597 |  0:01:18s\n",
            "epoch 41 | loss: 0.16439 | val_0_accuracy: 0.93676 |  0:01:20s\n",
            "epoch 42 | loss: 0.16139 | val_0_accuracy: 0.92954 |  0:01:22s\n",
            "epoch 43 | loss: 0.16287 | val_0_accuracy: 0.93419 |  0:01:24s\n",
            "epoch 44 | loss: 0.16169 | val_0_accuracy: 0.93825 |  0:01:26s\n",
            "epoch 45 | loss: 0.16267 | val_0_accuracy: 0.9333  |  0:01:28s\n",
            "epoch 46 | loss: 0.16372 | val_0_accuracy: 0.93617 |  0:01:30s\n",
            "epoch 47 | loss: 0.16297 | val_0_accuracy: 0.93746 |  0:01:32s\n",
            "epoch 48 | loss: 0.15754 | val_0_accuracy: 0.92835 |  0:01:34s\n",
            "epoch 49 | loss: 0.15808 | val_0_accuracy: 0.93617 |  0:01:35s\n",
            "epoch 50 | loss: 0.16357 | val_0_accuracy: 0.92083 |  0:01:37s\n",
            "epoch 51 | loss: 0.16549 | val_0_accuracy: 0.93102 |  0:01:39s\n",
            "epoch 52 | loss: 0.15947 | val_0_accuracy: 0.93112 |  0:01:41s\n",
            "epoch 53 | loss: 0.15858 | val_0_accuracy: 0.92934 |  0:01:43s\n",
            "epoch 54 | loss: 0.15745 | val_0_accuracy: 0.93112 |  0:01:45s\n",
            "\n",
            "Early stopping occurred at epoch 54 with best_epoch = 44 and best_val_0_accuracy = 0.93825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TabNet Performance:\n",
            "Accuracy:  0.9382\n",
            "Precision: 0.9471\n",
            "Recall:    0.9279\n",
            "F1 Score:  0.9374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell C: Layer 1 - Enhanced Random Forest and XGBoost Base Models\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import numpy as np\n",
        "\n",
        "# Initialize enhanced base models with better hyperparameters\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=20,\n",
        "    min_samples_split=3,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    gamma=0.1,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "\n",
        "# Train base models and get out-of-fold predictions (using 10-fold CV for better generalization)\n",
        "# Use existing X_train and y_train from your preprocessing\n",
        "rf_train_preds = cross_val_predict(rf_model, X_train, y_train, cv=10, method='predict_proba')\n",
        "xgb_train_preds = cross_val_predict(xgb_model, X_train, y_train, cv=10, method='predict_proba')\n",
        "\n",
        "# Fit models on full training set for test predictions\n",
        "rf_model.fit(X_train, y_train)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Get test predictions (both probability classes for richer information)\n",
        "# Use existing X_test from your preprocessing\n",
        "rf_test_preds = rf_model.predict_proba(X_test)\n",
        "xgb_test_preds = xgb_model.predict_proba(X_test)\n",
        "\n",
        "# Create enhanced stacked features with both probability classes\n",
        "X_train_stacked = np.column_stack([\n",
        "    rf_train_preds[:, 0], rf_train_preds[:, 1],\n",
        "    xgb_train_preds[:, 0], xgb_train_preds[:, 1]\n",
        "])\n",
        "\n",
        "X_test_stacked = np.column_stack([\n",
        "    rf_test_preds[:, 0], rf_test_preds[:, 1],\n",
        "    xgb_test_preds[:, 0], xgb_test_preds[:, 1]\n",
        "])\n",
        "\n",
        "print(f\"Stacked training features shape: {X_train_stacked.shape}\")\n",
        "print(f\"Stacked test features shape: {X_test_stacked.shape}\")"
      ],
      "metadata": {
        "id": "RnrdjkqbJdSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8887e2e3-732a-46cc-f4e3-e12e1bfb511f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacked training features shape: (40417, 4)\n",
            "Stacked test features shape: (10105, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell D: Layer 2 - Enhanced CNN 1D Meta-learner (using existing tensors structure)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class Enhanced_CNN1D_MetaLearner(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Enhanced_CNN1D_MetaLearner, self).__init__()\n",
        "        # Enhanced convolution layers\n",
        "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=2, padding=1)\n",
        "\n",
        "        # Batch normalization for stable training\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        # Global pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        # Enhanced fully connected layers\n",
        "        self.fc1 = nn.Linear(512, 256)  # 256*2 for avg+max pooling\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "\n",
        "        self.out = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "        # Convolutional layers with batch norm and activation\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        # Global pooling (both avg and max)\n",
        "        avg_pool = self.global_avg_pool(x)\n",
        "        max_pool = self.global_max_pool(x)\n",
        "\n",
        "        # Concatenate pooled features\n",
        "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = F.relu(self.fc3(x))\n",
        "\n",
        "        return self.out(x)\n",
        "\n",
        "# Convert stacked features to PyTorch tensors (following your existing pattern)\n",
        "X_train_stacked_tensor = torch.tensor(X_train_stacked, dtype=torch.float32)\n",
        "X_test_stacked_tensor = torch.tensor(X_test_stacked, dtype=torch.float32)\n",
        "\n",
        "# Use existing y_train_tensor and y_test_tensor from your preprocessing\n",
        "train_stacked_ds = TensorDataset(X_train_stacked_tensor, y_train_tensor)\n",
        "test_stacked_ds = TensorDataset(X_test_stacked_tensor, y_test_tensor)\n",
        "train_stacked_dl = DataLoader(train_stacked_ds, batch_size=64, shuffle=True)\n",
        "test_stacked_dl = DataLoader(test_stacked_ds, batch_size=64)\n",
        "\n",
        "# Initialize enhanced meta-learner\n",
        "meta_cnn = Enhanced_CNN1D_MetaLearner(X_train_stacked.shape[1])\n",
        "\n",
        "# Enhanced training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(meta_cnn.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=15, factor=0.5)\n",
        "\n",
        "# Training with early stopping\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "patience = 25\n",
        "\n",
        "for epoch in range(100):\n",
        "    meta_cnn.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for xb, yb in train_stacked_dl:\n",
        "        optimizer.zero_grad()\n",
        "        preds = meta_cnn(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(meta_cnn.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    meta_cnn.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_stacked_dl:\n",
        "            preds = meta_cnn(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_stacked_dl)\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = meta_cnn.state_dict().copy()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss/len(train_stacked_dl):.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "# Load best model\n",
        "meta_cnn.load_state_dict(best_model_state)\n",
        "print(\"Enhanced CNN 1D meta-learner training completed\")"
      ],
      "metadata": {
        "id": "TV_MDtoYJd1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c70865-5d96-42c9-afd6-f2ea99c31876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train Loss: 0.1168, Val Loss: 0.1156\n",
            "Epoch 40: Train Loss: 0.1148, Val Loss: 0.1119\n",
            "Epoch 60: Train Loss: 0.1151, Val Loss: 0.1112\n",
            "Epoch 80: Train Loss: 0.1142, Val Loss: 0.1124\n",
            "Early stopping at epoch 88\n",
            "Enhanced CNN 1D meta-learner training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell E: Evaluation and Metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Get final predictions using existing y_test from your preprocessing\n",
        "meta_cnn.eval()\n",
        "with torch.no_grad():\n",
        "    final_preds = meta_cnn(X_test_stacked_tensor)\n",
        "    y_pred_final = final_preds.argmax(dim=1).numpy()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_final)\n",
        "precision = precision_score(y_test, y_pred_final)\n",
        "recall = recall_score(y_test, y_pred_final)\n",
        "f1 = f1_score(y_test, y_pred_final)\n",
        "\n",
        "print(\"=== ENHANCED STACKING MODEL PERFORMANCE ===\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(\"\\n=== Detailed Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_final))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAXIiXf3We_0",
        "outputId": "9601c6ef-b25c-4cf6-be72-25977ebf7fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENHANCED STACKING MODEL PERFORMANCE ===\n",
            "Accuracy:  0.9567\n",
            "Precision: 0.9590\n",
            "Recall:    0.9537\n",
            "F1-Score:  0.9564\n",
            "\n",
            "=== Detailed Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96      5071\n",
            "           1       0.96      0.95      0.96      5034\n",
            "\n",
            "    accuracy                           0.96     10105\n",
            "   macro avg       0.96      0.96      0.96     10105\n",
            "weighted avg       0.96      0.96      0.96     10105\n",
            "\n"
          ]
        }
      ]
    }
  ]
}